{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f403e073",
   "metadata": {},
   "source": [
    "# LangGraph translation demo - Multi-Step Pipeline\n",
    "\n",
    "This notebook demonstrates the enhanced **multi-step \"generate-and-filter\"** translation pipeline.\n",
    "\n",
    "The new pipeline has 6 stages:\n",
    "1. **analyse_sense** - Understand semantic nuances\n",
    "2. **translate_definition** - Translate definition with context\n",
    "3. **translate_all_lemmas** - Direct translation of each lemma\n",
    "4. **expand_synonyms** - Broaden candidate pool in target language\n",
    "5. **filter_synonyms** - Quality check to remove imperfect matches\n",
    "6. **assemble_result** - Combine all outputs into final synset\n",
    "\n",
    "This approach generates high-quality synsets (sets of synonymous literals) rather than a single \"headword\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ce77d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "DATA_PATH = Path(\"../examples/serbian_english_synset_pairs_enhanced.json\")\n",
    "with DATA_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "pairs = dataset[\"pairs\"]\n",
    "metadata = dataset.get(\"metadata\", {})\n",
    "\n",
    "print(f\"Loaded {len(pairs)} pairs from {DATA_PATH}\")\n",
    "print(\"Metadata snapshot:\")\n",
    "pprint(metadata)\n",
    "\n",
    "sample_pair = pairs[0]\n",
    "print(\"\\nFirst pair keys:\", list(sample_pair.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075401dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint({\n",
    "    \"english_id\": sample_pair.get(\"english_id\"),\n",
    "    \"english_lemmas\": sample_pair.get(\"english_lemmas\"),\n",
    "    \"english_definition\": sample_pair.get(\"english_definition\"),\n",
    "    \"english_examples\": sample_pair.get(\"english_examples\"),\n",
    "    \"serbian_synonyms\": sample_pair.get(\"serbian_synonyms\"),\n",
    "    \"serbian_definition\": sample_pair.get(\"serbian_definition\"),\n",
    "    \"serbian_usage\": sample_pair.get(\"serbian_usage\"),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee1d326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ollama\n",
    "import wordnet_autotranslate.pipelines.langgraph_translation_pipeline as lg_module\n",
    "\n",
    "lg_module = importlib.reload(lg_module)\n",
    "LangGraphTranslationPipeline = lg_module.LangGraphTranslationPipeline\n",
    "\n",
    "PREFERRED_OLLAMA_MODEL = \"gpt-oss:120b\"\n",
    "OLLAMA_TIMEOUT = 180  # seconds\n",
    "OLLAMA_TEMPERATURE = 0.0\n",
    "\n",
    "try:\n",
    "    model_list_response = ollama.list()\n",
    "    available_models = {item.model for item in model_list_response.models}\n",
    "except Exception as exc:  # pragma: no cover - depends on local runtime\n",
    "    raise RuntimeError(\n",
    "        \"Could not reach the local Ollama daemon. Start it with `ollama serve`.\"\n",
    "    ) from exc\n",
    "\n",
    "if not available_models:\n",
    "    raise RuntimeError(\n",
    "        \"No Ollama models are installed. Pull one with `ollama pull <model>` before running this cell.\"\n",
    "    )\n",
    "\n",
    "if PREFERRED_OLLAMA_MODEL in available_models:\n",
    "    ollama_model = PREFERRED_OLLAMA_MODEL\n",
    "else:\n",
    "    ollama_model = sorted(available_models)[0]\n",
    "    print(\n",
    "        f\"Preferred model '{PREFERRED_OLLAMA_MODEL}' not found. \"\n",
    "        f\"Falling back to '{ollama_model}'.\"\n",
    "    )\n",
    "\n",
    "pipeline = LangGraphTranslationPipeline(\n",
    "    source_lang=\"en\",\n",
    "    target_lang=\"sr\",\n",
    "    model=ollama_model,\n",
    "    temperature=OLLAMA_TEMPERATURE,\n",
    "    timeout=OLLAMA_TIMEOUT,\n",
    ")\n",
    "\n",
    "print(f\"Using Ollama model: {ollama_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab25d087",
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_limit = 200\n",
    "\n",
    "\n",
    "def preview_text(text: str | None, limit: int = preview_limit) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return text[:limit] + (\"â€¦ [truncated]\" if len(text) > limit else \"\")\n",
    "\n",
    "\n",
    "synset_input = {\n",
    "    \"id\": sample_pair.get(\"english_id\"),\n",
    "    \"english_id\": sample_pair.get(\"english_id\"),\n",
    "    \"lemmas\": sample_pair.get(\"english_lemmas\", []),\n",
    "    \"definition\": sample_pair.get(\"english_definition\", \"\"),\n",
    "    \"examples\": sample_pair.get(\"english_examples\", []),\n",
    "    \"pos\": sample_pair.get(\"english_pos\"),\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRANSLATING SYNSET WITH MULTI-STEP PIPELINE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Input synset ID: {synset_input.get('id')}\")\n",
    "print(f\"English lemmas: {synset_input.get('lemmas')}\")\n",
    "print(f\"Definition: {synset_input.get('definition')}\")\n",
    "print(\"\\nRunning through 7-stage pipeline...\")\n",
    "print(\"  Stage 1: analyse_sense\")\n",
    "print(\"  Stage 2: translate_definition\")\n",
    "print(\"  Stage 3: translate_all_lemmas (NEW)\")\n",
    "print(\"  Stage 4: expand_synonyms (NEW)\")\n",
    "print(\"  Stage 5: filter_synonyms (NEW)\")\n",
    "print(\"  Stage 6: review_definition_quality (NEW)\")\n",
    "print(\"  Stage 7: assemble_result\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "result = pipeline.translate_synset(synset_input)\n",
    "\n",
    "translation = result.get(\"translation\", \"\")\n",
    "definition_translation = result.get(\"definition_translation\", \"\")\n",
    "translated_synonyms = result.get(\"translated_synonyms\", [])\n",
    "examples = result.get(\"examples\", [])\n",
    "notes = result.get(\"notes\")\n",
    "curator_summary = result.get(\"curator_summary\", \"\")\n",
    "raw_response = result.get(\"raw_response\", \"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Representative literal: {translation}\")\n",
    "print(f\"Final synset ({len(translated_synonyms)} literals): {translated_synonyms}\")\n",
    "print(f\"Example count: {len(examples)}\")\n",
    "print(f\"Definition translation length: {len(definition_translation)} characters\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CURATOR SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(curator_summary)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DEFINITION TRANSLATION\")\n",
    "print(\"=\" * 70)\n",
    "print(definition_translation)\n",
    "\n",
    "# Show pipeline progression\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PIPELINE STAGE DETAILS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "payload = result.get(\"payload\", {})\n",
    "initial_payload = payload.get(\"initial_translation\", {})\n",
    "expansion_payload = payload.get(\"expansion\", {})\n",
    "filtering_payload = payload.get(\"filtering\", {})\n",
    "definition_quality_payload = payload.get(\"definition_quality\", {}) or {}\n",
    "\n",
    "initial_translations = initial_payload.get(\"initial_translations\", [])\n",
    "expanded_synonyms = expansion_payload.get(\"expanded_synonyms\", [])\n",
    "filtered_synonyms = filtering_payload.get(\"filtered_synonyms\", [])\n",
    "\n",
    "print(f\"\\nğŸ“Š Stage 3 - Initial Translations ({len(initial_translations)} lemmas):\")\n",
    "for i, trans in enumerate(initial_translations, 1):\n",
    "    print(f\"  {i}. {trans}\")\n",
    "\n",
    "print(f\"\\nğŸ” Stage 4 - Expanded Candidates ({len(expanded_synonyms)} synonyms):\")\n",
    "for i, syn in enumerate(expanded_synonyms, 1):\n",
    "    print(f\"  {i}. {syn}\")\n",
    "\n",
    "print(f\"\\nâœ… Stage 5 - Filtered Results ({len(filtered_synonyms)} final literals):\")\n",
    "for i, lit in enumerate(filtered_synonyms, 1):\n",
    "    print(f\"  {i}. {lit}\")\n",
    "\n",
    "# Show what was removed during filtering\n",
    "removed = set(expanded_synonyms) - set(filtered_synonyms)\n",
    "if removed:\n",
    "    print(f\"\\nâŒ Removed during filtering ({len(removed)} items):\")\n",
    "    for item in sorted(removed):\n",
    "        print(f\"  - {item}\")\n",
    "else:\n",
    "    print(\"\\nâœ“ No items removed during filtering (all candidates passed validation)\")\n",
    "\n",
    "definition_quality_status_raw = (definition_quality_payload.get(\"status\") or \"unknown\").lower()\n",
    "definition_quality_status = definition_quality_status_raw.replace(\"_\", \" \").title()\n",
    "definition_quality_issues = definition_quality_payload.get(\"issues\") or []\n",
    "definition_quality_revision = definition_quality_payload.get(\"revised_definition\")\n",
    "definition_quality_notes = definition_quality_payload.get(\"notes\")\n",
    "definition_quality_revision_text = definition_quality_revision.strip() if isinstance(definition_quality_revision, str) else \"\"\n",
    "definition_quality_revision_applied = bool(definition_quality_revision_text) and definition_quality_status_raw == \"needs_revision\"\n",
    "\n",
    "print(f\"\\nğŸ§ª Stage 6 - Definition Quality Review: {definition_quality_status}\")\n",
    "if definition_quality_issues:\n",
    "    for issue in definition_quality_issues:\n",
    "        issue_type = issue.get(\"type\", \"unknown\")\n",
    "        issue_message = issue.get(\"message\", \"\")\n",
    "        print(f\"  - {issue_type}: {issue_message}\")\n",
    "else:\n",
    "    print(\"  No issues flagged.\")\n",
    "\n",
    "if definition_quality_revision_text:\n",
    "    print(\"  Revised definition candidate:\")\n",
    "    print(f\"    {preview_text(definition_quality_revision_text, 200)}\")\n",
    "\n",
    "if definition_quality_notes:\n",
    "    print(f\"  Notes: {preview_text(definition_quality_notes, 200)}\")\n",
    "\n",
    "print(\"\\nğŸ Stage 7 - assemble_result â†’ final synset ready for export\")\n",
    "\n",
    "if examples:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"EXAMPLES\")\n",
    "    print(\"=\" * 70)\n",
    "    for i, ex in enumerate(examples[:3], 1):\n",
    "        print(f\"{i}. {preview_text(ex)}\")\n",
    "    if len(examples) > 3:\n",
    "        print(f\"... ({len(examples)} total)\")\n",
    "\n",
    "if notes:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"NOTES\")\n",
    "    print(\"=\" * 70)\n",
    "    print(preview_text(notes))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STAGE LOGS (Preview)\")\n",
    "print(\"=\" * 70)\n",
    "logs = result.get(\"payload\", {}).get(\"logs\", {})\n",
    "for stage, log in logs.items():\n",
    "    if not log:\n",
    "        continue\n",
    "    print(f\"\\n[{stage.upper()}]\")\n",
    "    print(f\"  Prompt preview: {preview_text(log.get('prompt'), 150)}\")\n",
    "    print(f\"  Response preview: {preview_text(log.get('raw_response_preview'), 150)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FULL RAW RESPONSE (Last Stage)\")\n",
    "print(\"=\" * 70)\n",
    "print(preview_text(raw_response, 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b2e679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract final synset for simple JSON output\n",
    "# The new pipeline produces a clean list of validated synonyms\n",
    "\n",
    "simple_output = {\n",
    "    \"english_id\": synset_input.get(\"id\"),\n",
    "    \"representative_literal\": translation,  # First literal (for convenience)\n",
    "    \"synset_literals\": translated_synonyms,  # The actual synset\n",
    "    \"literal_count\": len(translated_synonyms),\n",
    "    \"definition_translation\": definition_translation,\n",
    "    \"definition_quality\": {\n",
    "        \"status\": definition_quality_status_raw,\n",
    "        \"issues\": definition_quality_issues,\n",
    "        \"revision_applied\": definition_quality_revision_applied,\n",
    "    },\n",
    "    \"pipeline_stages\": {\n",
    "        \"initial_translations\": len(initial_translations),\n",
    "        \"expanded_candidates\": len(expanded_synonyms),\n",
    "        \"filtered_results\": len(filtered_synonyms),\n",
    "        \"definition_quality_review\": definition_quality_status_raw,\n",
    "        \"removal_rate\": f\"{len(removed)}/{len(expanded_synonyms)}\" if expanded_synonyms else \"0/0\"\n",
    "    },\n",
    "}\n",
    "\n",
    "output_path = Path(\"simple_translation_output.json\")\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as fp:\n",
    "    json.dump(simple_output, fp, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SIMPLE JSON OUTPUT\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Saved to: {output_path.resolve()}\\n\")\n",
    "print(json.dumps(simple_output, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f0e322",
   "metadata": {},
   "source": [
    "## Understanding the Multi-Step Pipeline\n",
    "\n",
    "### Generate â†’ Expand â†’ Filter â†’ Review Workflow\n",
    "\n",
    "The pipeline now uses a **4-stage synonym workflow** before final assembly:\n",
    "\n",
    "1. **Generate Initial** (`translate_all_lemmas`): Direct translation of each English lemma\n",
    "2. **Expand** (`expand_synonyms`): Find additional synonyms in the target language\n",
    "3. **Filter** (`filter_synonyms`): Remove candidates with imperfect sense alignment\n",
    "4. **Review Definition** (`review_definition_quality`): Polish translated gloss for grammar and style\n",
    "\n",
    "### Benefits\n",
    "\n",
    "âœ… **Higher Quality**: Four-stage validation ensures precision\n",
    "âœ… **Broader Coverage**: Expansion finds native synonyms, not just literal translations\n",
    "âœ… **Stylistic Assurance**: Definition review catches grammar and circularity issues\n",
    "âœ… **Traceability**: Full audit trail showing progression\n",
    "âœ… **No Headword**: Output is a true synset (set of synonymous literals)\n",
    "\n",
    "### Next steps\n",
    "\n",
    "- Translate multiple synsets: `pipeline.translate(list_of_synsets)`\n",
    "- Use streaming for large batches: `pipeline.translate_stream(synsets)`\n",
    "- Inspect `filtering_payload` to see which candidates were rejected\n",
    "- Review `definition_quality` payload for grammar/style diagnostics\n",
    "- Compare quality against the old single-step approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f07035",
   "metadata": {},
   "source": [
    "## ğŸ” Accessing Full LLM Outputs (Untruncated)\n",
    "\n",
    "The pipeline preserves **two versions** of LLM call data:\n",
    "\n",
    "- **`result[\"payload\"][\"logs\"]`** - Truncated summaries for quick viewing (500 chars)\n",
    "- **`result[\"payload\"][\"calls\"]`** - **Full, untruncated** LLM interactions\n",
    "\n",
    "The truncation exists to prevent memory bloat when processing hundreds of synsets, but you can always access the complete data via the `calls` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7331f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: View truncated logs (quick summary)\n",
    "print(\"=== TRUNCATED LOGS (for quick viewing) ===\\n\")\n",
    "print(\"Filtering stage log (truncated):\")\n",
    "truncated_log = result[\"payload\"][\"logs\"][\"filtering\"]\n",
    "print(f\"  Raw response preview: {truncated_log.get('raw_response_preview', 'N/A')[:100]}...\")\n",
    "print(f\"  (Response truncated at {len(truncated_log.get('raw_response_preview', ''))} chars)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8edab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Access FULL untruncated data\n",
    "print(\"\\n=== FULL UNTRUNCATED CALL DATA ===\\n\")\n",
    "print(\"Filtering stage call (complete):\")\n",
    "full_call = result[\"payload\"][\"calls\"][\"filtering\"]\n",
    "print(f\"  Stage: {full_call['stage']}\")\n",
    "print(f\"  Full raw response length: {len(full_call['raw_response'])} chars\")\n",
    "print(f\"  Full raw response:\\n{full_call['raw_response']}\")\n",
    "print(f\"\\n  Parsed payload: {full_call['payload']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bef6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save full logs to file for later analysis\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path(\"output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "full_logs_path = output_dir / \"full_llm_logs.json\"\n",
    "\n",
    "# Extract all full calls for detailed analysis\n",
    "full_logs = {\n",
    "    \"synset_id\": result[\"source\"][\"id\"],\n",
    "    \"translation\": result[\"translation\"],\n",
    "    \"all_stages\": {\n",
    "        stage: {\n",
    "            \"prompt\": call.get(\"prompt\", \"\"),\n",
    "            \"raw_response\": call.get(\"raw_response\", \"\"),\n",
    "            \"parsed_payload\": call.get(\"payload\", {}),\n",
    "            \"messages\": call.get(\"messages\", [])\n",
    "        }\n",
    "        for stage, call in result[\"payload\"][\"calls\"].items()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(full_logs_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(full_logs, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… Full untruncated logs saved to: {full_logs_path}\")\n",
    "print(f\"ğŸ“Š File size: {full_logs_path.stat().st_size:,} bytes\")\n",
    "print(f\"\\nğŸ’¡ Tip: These logs include:\")\n",
    "print(\"   - Complete prompts for each stage\")\n",
    "print(\"   - Full raw LLM responses (no truncation)\")\n",
    "print(\"   - Parsed payloads with validation results\")\n",
    "print(\"   - Complete message history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bac52d6",
   "metadata": {},
   "source": [
    "### ğŸ“ Why Truncate Logs?\n",
    "\n",
    "**Memory & Performance Reasons:**\n",
    "\n",
    "1. **Memory Bloat**: When processing 1,000+ synsets, full responses can consume GBs of RAM\n",
    "2. **Serialization Size**: Saving results to JSON becomes impractical with full responses\n",
    "3. **Quick Inspection**: Truncated logs allow fast debugging without scrolling through pages\n",
    "\n",
    "**Best Practice:**\n",
    "- Use `payload[\"logs\"]` for quick debugging during development\n",
    "- Use `payload[\"calls\"]` to save full data for critical synsets\n",
    "- Export full logs to separate files for detailed analysis (as shown above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a68de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the log utilities for easier log management\n",
    "from wordnet_autotranslate.utils.log_utils import save_full_logs, analyze_stage_lengths\n",
    "\n",
    "# Save full logs with one function call\n",
    "log_path = save_full_logs(result, output_path=\"output/full_logs_example.json\")\n",
    "print(f\"âœ… Saved to: {log_path}\")\n",
    "\n",
    "# Analyze response sizes per stage\n",
    "print(\"\\nğŸ“Š Response sizes by stage:\")\n",
    "lengths = analyze_stage_lengths(result)\n",
    "for stage, length in sorted(lengths.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {stage:20} {length:>8,} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e838cf6c",
   "metadata": {},
   "source": [
    "## ğŸ¯ New Feature: Per-Word Confidence Levels\n",
    "\n",
    "The filtering stage now provides **confidence levels for each individual synonym**, allowing you to:\n",
    "- Assess quality at the word level\n",
    "- Filter results by confidence threshold\n",
    "- Understand which synonyms are most reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c52b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access per-word confidence levels from the filtering stage\n",
    "filtering_payload = result[\"payload\"][\"filtering\"]\n",
    "\n",
    "print(\"=== PER-WORD CONFIDENCE LEVELS ===\\n\")\n",
    "\n",
    "# Get the confidence dictionary\n",
    "confidence_by_word = filtering_payload.get(\"confidence_by_word\", {})\n",
    "\n",
    "if confidence_by_word:\n",
    "    print(f\"Filtered synonyms with individual confidence levels:\")\n",
    "    for word, conf in confidence_by_word.items():\n",
    "        emoji = \"ğŸŸ¢\" if conf == \"high\" else \"ğŸŸ¡\" if conf == \"medium\" else \"ğŸ”´\"\n",
    "        print(f\"  {emoji} {word:20} â†’ {conf}\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total = len(confidence_by_word)\n",
    "    high = sum(1 for c in confidence_by_word.values() if c == \"high\")\n",
    "    medium = sum(1 for c in confidence_by_word.values() if c == \"medium\")\n",
    "    low = sum(1 for c in confidence_by_word.values() if c == \"low\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Confidence Distribution:\")\n",
    "    print(f\"  ğŸŸ¢ High:   {high}/{total} ({high/total*100:.1f}%)\")\n",
    "    print(f\"  ğŸŸ¡ Medium: {medium}/{total} ({medium/total*100:.1f}%)\")\n",
    "    print(f\"  ğŸ”´ Low:    {low}/{total} ({low/total*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"âš ï¸ No per-word confidence data available\")\n",
    "    print(\"(This field is optional - older pipeline versions may not include it)\")\n",
    "\n",
    "# Show overall filtering confidence\n",
    "overall_confidence = filtering_payload.get(\"confidence\", \"N/A\")\n",
    "print(f\"\\nğŸ¯ Overall filtering confidence: {overall_confidence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d72673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical use: Filter synonyms by confidence threshold\n",
    "print(\"=== FILTERING BY CONFIDENCE THRESHOLD ===\\n\")\n",
    "\n",
    "confidence_by_word = filtering_payload.get(\"confidence_by_word\", {})\n",
    "\n",
    "if confidence_by_word:\n",
    "    # Get only high-confidence synonyms\n",
    "    high_confidence = [word for word, conf in confidence_by_word.items() if conf == \"high\"]\n",
    "    print(f\"ğŸŸ¢ High-confidence only ({len(high_confidence)} words):\")\n",
    "    for word in high_confidence:\n",
    "        print(f\"  â€¢ {word}\")\n",
    "    \n",
    "    # Get high + medium confidence\n",
    "    reliable = [word for word, conf in confidence_by_word.items() if conf in [\"high\", \"medium\"]]\n",
    "    print(f\"\\nğŸŸ¢ğŸŸ¡ High + Medium confidence ({len(reliable)} words):\")\n",
    "    for word in reliable:\n",
    "        conf = confidence_by_word[word]\n",
    "        print(f\"  â€¢ {word} ({conf})\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ Use Case Examples:\")\n",
    "    print(\"  - Strict quality: Use only 'high' confidence words\")\n",
    "    print(\"  - Balanced: Use 'high' + 'medium' (default)\")\n",
    "    print(\"  - Exploratory: Include all levels for broader coverage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b814da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the improved filtering prompt that generated this result\n",
    "print(\"=== IMPROVED FILTERING PROMPT ===\\n\")\n",
    "\n",
    "filtering_call = result[\"payload\"][\"calls\"][\"filtering\"]\n",
    "prompt = filtering_call.get(\"prompt\", \"\")\n",
    "\n",
    "# Show key parts of the new prompt\n",
    "print(\"Key improvements in the filtering prompt:\")\n",
    "print(\"\\n1ï¸âƒ£ Balanced Approach:\")\n",
    "print(\"   âœ“ Preserve core concept while allowing natural variation\")\n",
    "print(\"   âœ“ Prefer idiomatic expressions over literal translations\")\n",
    "print(\"\\n2ï¸âƒ£ Cultural Sensitivity:\")\n",
    "print(\"   âœ“ Choose words typical in modern usage\")\n",
    "print(\"   âœ“ Include culturally appropriate expressions\")\n",
    "print(\"\\n3ï¸âƒ£ Flexibility:\")\n",
    "print(\"   âœ“ Allow abstract/concrete variants if natives use them\")\n",
    "print(\"   âœ“ Reject only clearly different concepts\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Full Filtering Prompt:\")\n",
    "print(\"=\" * 70)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630363c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine what was removed during filtering and why\n",
    "print(\"=== FILTERING DECISIONS ===\\n\")\n",
    "\n",
    "removed_items = filtering_payload.get(\"removed\", [])\n",
    "\n",
    "if removed_items:\n",
    "    print(f\"âŒ Removed candidates ({len(removed_items)} items):\\n\")\n",
    "    for item in removed_items:\n",
    "        word = item.get(\"word\", \"?\")\n",
    "        reason = item.get(\"reason\", \"No reason provided\")\n",
    "        print(f\"  â€¢ {word:20} â†’ {reason}\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ These removals show the LLM's understanding of:\")\n",
    "    print(\"   - Which words are too broad/narrow\")\n",
    "    print(\"   - Which don't fit the cultural context\")\n",
    "    print(\"   - Which belong to different concepts\")\n",
    "else:\n",
    "    print(\"âœ… No candidates were removed!\")\n",
    "    print(\"   All expanded synonyms passed the filtering stage.\")\n",
    "\n",
    "# Show the progression\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PIPELINE PROGRESSION\")\n",
    "print(\"=\" * 70)\n",
    "expanded = expansion_payload.get(\"expanded_synonyms\", [])\n",
    "filtered = filtering_payload.get(\"filtered_synonyms\", [])\n",
    "\n",
    "print(f\"\\nğŸ“Š Stage 4 - Expanded: {len(expanded)} candidates\")\n",
    "print(f\"   {', '.join(expanded)}\")\n",
    "print(f\"\\nâœ… Stage 5 - Filtered: {len(filtered)} final synonyms\")\n",
    "print(f\"   {', '.join(filtered)}\")\n",
    "print(f\"\\nğŸ“‰ Removal rate: {len(expanded) - len(filtered)}/{len(expanded)} candidates removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d431fd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”„ Additional Synset Translations\n",
    "\n",
    "Let's translate four more synsets to demonstrate how the pipeline handles different types of words and semantic relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df97d91c",
   "metadata": {},
   "source": [
    "### ğŸ”¹ Synset 2: \"happiness\" (abstract emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f767fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show next 4 synsets from the dataset and prepare them for translation\n",
    "print(\"ğŸ“‹ Next 4 synsets available for translation:\\n\")\n",
    "\n",
    "synsets_to_translate = []\n",
    "\n",
    "for i in range(1, min(5, len(pairs))):\n",
    "    pair = pairs[i]\n",
    "    \n",
    "    # Build synset structure from pair data (use 'lemmas' field for pipeline compatibility)\n",
    "    synset = {\n",
    "        \"id\": pair[\"english_id\"],\n",
    "        \"pos\": pair[\"english_pos\"],\n",
    "        \"lexname\": pair.get(\"english_domain\", \"\"),\n",
    "        \"definition\": pair[\"english_definition\"],\n",
    "        \"examples\": pair.get(\"english_examples\", []),\n",
    "        \"lemmas\": pair[\"english_lemmas\"],  # Pipeline looks for 'lemmas' field\n",
    "        \"synonyms\": pair[\"english_lemmas\"],  # Keep for display\n",
    "        \"ili\": \"\",  # Not in this dataset\n",
    "        \"topic_domains\": []\n",
    "    }\n",
    "    \n",
    "    synsets_to_translate.append(synset)\n",
    "    \n",
    "    print(f\"{i+1}. {synset['id']} - {', '.join(synset['synonyms'][:2])}\")\n",
    "    print(f\"   POS: {synset['pos']} | Domain: {synset.get('lexname', 'N/A')}\")\n",
    "    print(f\"   Definition: {synset['definition'][:80]}...\")\n",
    "    print()\n",
    "\n",
    "print(f\"âœ… Prepared {len(synsets_to_translate)} synsets for translation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8257099",
   "metadata": {},
   "source": [
    "### ğŸ”¹ Synset 2: \"condiment\" (concrete noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3bdd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate synset 2: condiment\n",
    "synset_2 = synsets_to_translate[0]\n",
    "\n",
    "print(f\"ğŸ”„ Translating: {', '.join(synset_2['synonyms'])}\")\n",
    "print(f\"   Definition: {synset_2['definition']}\\n\")\n",
    "\n",
    "result_2 = pipeline.translate_synset(synset_2)\n",
    "\n",
    "# Extract payloads (use correct field names)\n",
    "filtering_payload_2 = result_2[\"payload\"][\"filtering\"]\n",
    "expansion_payload_2 = result_2[\"payload\"][\"expansion\"]\n",
    "\n",
    "print(\"\\nâœ… Translation complete!\")\n",
    "print(f\"   Filtered: {', '.join(filtering_payload_2['filtered_synonyms'])}\")\n",
    "print(f\"   Confidence: {filtering_payload_2['confidence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5ee3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Ollama status and model availability\n",
    "import ollama\n",
    "\n",
    "try:\n",
    "    # Test if Ollama is responding\n",
    "    test_response = ollama.chat(\n",
    "        model=PREFERRED_OLLAMA_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Respond with just: OK\"}],\n",
    "        options={\"temperature\": 0.0}\n",
    "    )\n",
    "    print(f\"âœ… Ollama is responding\")\n",
    "    print(f\"   Model: {PREFERRED_OLLAMA_MODEL}\")\n",
    "    print(f\"   Test response: {test_response['message']['content']}\\n\")\n",
    "    \n",
    "    # Try a simple JSON generation test\n",
    "    json_test = ollama.chat(\n",
    "        model=PREFERRED_OLLAMA_MODEL,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Return ONLY valid JSON (no markdown): {\\\"test\\\": \\\"value\\\"}\"\n",
    "        }],\n",
    "        options={\"temperature\": 0.0}\n",
    "    )\n",
    "    print(f\"JSON test response:\")\n",
    "    print(json_test['message']['content'][:200])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Ollama error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9e2129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all stages in result_2\n",
    "print(\"=\" * 70)\n",
    "print(\"Checking all stages...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for stage_name, stage_data in result_2[\"payload\"].items():\n",
    "    if stage_name in [\"calls\", \"logs\"]:\n",
    "        continue\n",
    "    print(f\"\\n{stage_name}:\")\n",
    "    print(f\"  Type: {type(stage_data)}\")\n",
    "    if isinstance(stage_data, dict):\n",
    "        if \"error\" in stage_data:\n",
    "            print(f\"  âŒ Error: {stage_data['error']}\")\n",
    "        else:\n",
    "            print(f\"  Keys: {list(stage_data.keys())[:5]}\")  # Show first 5 keys\n",
    "    else:\n",
    "        print(f\"  Value: {str(stage_data)[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1ca6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis for synset 2\n",
    "print(\"=\" * 70)\n",
    "print(\"SYNSET 2 ANALYSIS: condiment\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Confidence breakdown\n",
    "confidence_by_word_2 = filtering_payload_2.get(\"confidence_by_word\", {})\n",
    "if confidence_by_word_2:\n",
    "    print(\"\\nğŸ¯ Per-word confidence:\")\n",
    "    for word, conf in confidence_by_word_2.items():\n",
    "        emoji_map = {\"high\": \"ğŸŸ¢\", \"medium\": \"ğŸŸ¡\", \"low\": \"ğŸ”´\"}\n",
    "        print(f\"   {emoji_map.get(conf, 'âšª')} {word:20} â†’ {conf}\")\n",
    "\n",
    "# Removed items\n",
    "removed_2 = filtering_payload_2.get(\"removed\", [])\n",
    "print(f\"\\nâŒ Removed: {len(removed_2)} candidates\")\n",
    "if removed_2:\n",
    "    for item in removed_2:\n",
    "        print(f\"   â€¢ {item.get('word', '?'):20} â†’ {item.get('reason', 'No reason')}\")\n",
    "\n",
    "# Pipeline progression\n",
    "expanded_2 = expansion_payload_2.get(\"expanded_synonyms\", [])\n",
    "filtered_2 = filtering_payload_2.get(\"filtered_synonyms\", [])\n",
    "print(f\"\\nğŸ“Š Expansion: {len(expanded_2)} â†’ Filtering: {len(filtered_2)}\")\n",
    "print(f\"   Before: {', '.join(expanded_2)}\")\n",
    "print(f\"   After:  {', '.join(filtered_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8fa62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show definition translation for synset 2\n",
    "print(\"=\" * 70)\n",
    "print(\"DEFINITION TRANSLATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "definition_payload_2 = result_2[\"payload\"][\"definition\"]\n",
    "\n",
    "print(f\"\\nğŸ‡¬ğŸ‡§ English definition:\")\n",
    "print(f\"   {synset_2['definition']}\\n\")\n",
    "\n",
    "print(f\"ğŸ‡·ğŸ‡¸ Serbian translation:\")\n",
    "print(f\"   {definition_payload_2['definition_translation']}\\n\")\n",
    "\n",
    "if definition_payload_2.get('notes'):\n",
    "    print(f\"ğŸ“ Translation notes:\")\n",
    "    print(f\"   {definition_payload_2['notes']}\\n\")\n",
    "\n",
    "# Compare with existing Serbian WordNet\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPARISON WITH EXISTING SERBIAN WORDNET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get the Serbian synset from the original pair\n",
    "serbian_pair = pairs[1]  # Same index as synset_2\n",
    "\n",
    "print(f\"\\nğŸ†• Our pipeline output:\")\n",
    "print(f\"   Synonyms: {', '.join(filtering_payload_2['filtered_synonyms'])}\")\n",
    "print(f\"   Confidence: {filtering_payload_2['confidence']}\\n\")\n",
    "\n",
    "print(f\"ğŸ“š Existing Serbian WordNet:\")\n",
    "print(f\"   Synset ID: {serbian_pair['serbian_id']}\")\n",
    "print(f\"   Synonyms: {', '.join(serbian_pair['serbian_synonyms'])}\")\n",
    "print(f\"   Definition: {serbian_pair['serbian_definition']}\\n\")\n",
    "\n",
    "# Find overlap\n",
    "our_words = set(filtering_payload_2['filtered_synonyms'])\n",
    "their_words = set(serbian_pair['serbian_synonyms'])\n",
    "\n",
    "overlap = our_words & their_words\n",
    "only_ours = our_words - their_words\n",
    "only_theirs = their_words - our_words\n",
    "\n",
    "print(f\"ğŸ”„ Comparison:\")\n",
    "print(f\"   âœ… Overlap: {', '.join(overlap) if overlap else 'None'}\")\n",
    "print(f\"   ğŸ†• Only in our output: {', '.join(only_ours) if only_ours else 'None'}\")\n",
    "print(f\"   ğŸ“š Only in existing: {', '.join(only_theirs) if only_theirs else 'None'}\")\n",
    "print(f\"   ğŸ“Š Match rate: {len(overlap)}/{len(their_words)} ({len(overlap)/len(their_words)*100:.0f}% of existing synset)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13791187",
   "metadata": {},
   "source": [
    "### ğŸ”¹ Synset 3: \"scatter, sprinkle\" (verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cb34bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate synset 3: scatter, sprinkle\n",
    "synset_3 = synsets_to_translate[1]\n",
    "\n",
    "print(f\"ğŸ”„ Translating: {', '.join(synset_3['synonyms'])}\")\n",
    "print(f\"   Definition: {synset_3['definition']}\\n\")\n",
    "\n",
    "result_3 = pipeline.translate_synset(synset_3)\n",
    "\n",
    "filtering_payload_3 = result_3[\"payload\"][\"filtering\"]\n",
    "expansion_payload_3 = result_3[\"payload\"][\"expansion\"]\n",
    "\n",
    "print(\"\\nâœ… Translation complete!\")\n",
    "print(f\"   Filtered: {', '.join(filtering_payload_3['filtered_synonyms'])}\")\n",
    "print(f\"   Confidence: {filtering_payload_3['confidence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3faeb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis for synset 3\n",
    "print(\"=\" * 70)\n",
    "print(\"SYNSET 3 ANALYSIS: scatter, sprinkle\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "confidence_by_word_3 = filtering_payload_3.get(\"confidence_by_word\", {})\n",
    "if confidence_by_word_3:\n",
    "    print(\"\\nğŸ¯ Per-word confidence:\")\n",
    "    for word, conf in confidence_by_word_3.items():\n",
    "        emoji_map = {\"high\": \"ğŸŸ¢\", \"medium\": \"ğŸŸ¡\", \"low\": \"ğŸ”´\"}\n",
    "        print(f\"   {emoji_map.get(conf, 'âšª')} {word:20} â†’ {conf}\")\n",
    "\n",
    "removed_3 = filtering_payload_3.get(\"removed\", [])\n",
    "print(f\"\\nâŒ Removed: {len(removed_3)} candidates\")\n",
    "if removed_3:\n",
    "    for item in removed_3:\n",
    "        print(f\"   â€¢ {item.get('word', '?'):20} â†’ {item.get('reason', 'No reason')}\")\n",
    "\n",
    "expanded_3 = expansion_payload_3.get(\"expanded_synonyms\", [])\n",
    "filtered_3 = filtering_payload_3.get(\"filtered_synonyms\", [])\n",
    "print(f\"\\nğŸ“Š Expansion: {len(expanded_3)} â†’ Filtering: {len(filtered_3)}\")\n",
    "print(f\"   Before: {', '.join(expanded_3)}\")\n",
    "print(f\"   After:  {', '.join(filtered_3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ed8769",
   "metadata": {},
   "source": [
    "### ğŸ”¹ Synset 4: \"pick, pluck\" (verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be84ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate synset 4: pick, pluck\n",
    "synset_4 = synsets_to_translate[2]\n",
    "\n",
    "print(f\"ğŸ”„ Translating: {', '.join(synset_4['synonyms'])}\")\n",
    "print(f\"   Definition: {synset_4['definition']}\\n\")\n",
    "\n",
    "result_4 = pipeline.translate_synset(synset_4)\n",
    "\n",
    "filtering_payload_4 = result_4[\"payload\"][\"filtering\"]\n",
    "expansion_payload_4 = result_4[\"payload\"][\"expansion\"]\n",
    "\n",
    "print(\"\\nâœ… Translation complete!\")\n",
    "print(f\"   Filtered: {', '.join(filtering_payload_4['filtered_synonyms'])}\")\n",
    "print(f\"   Confidence: {filtering_payload_4['confidence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac9b83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis for synset 4\n",
    "print(\"=\" * 70)\n",
    "print(\"SYNSET 4 ANALYSIS: pick, pluck\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "confidence_by_word_4 = filtering_payload_4.get(\"confidence_by_word\", {})\n",
    "if confidence_by_word_4:\n",
    "    print(\"\\nğŸ¯ Per-word confidence:\")\n",
    "    for word, conf in confidence_by_word_4.items():\n",
    "        emoji_map = {\"high\": \"ğŸŸ¢\", \"medium\": \"ğŸŸ¡\", \"low\": \"ğŸ”´\"}\n",
    "        print(f\"   {emoji_map.get(conf, 'âšª')} {word:20} â†’ {conf}\")\n",
    "\n",
    "removed_4 = filtering_payload_4.get(\"removed\", [])\n",
    "print(f\"\\nâŒ Removed: {len(removed_4)} candidates\")\n",
    "if removed_4:\n",
    "    for item in removed_4:\n",
    "        print(f\"   â€¢ {item.get('word', '?'):20} â†’ {item.get('reason', 'No reason')}\")\n",
    "\n",
    "expanded_4 = expansion_payload_4.get(\"expanded_synonyms\", [])\n",
    "filtered_4 = filtering_payload_4.get(\"filtered_synonyms\", [])\n",
    "print(f\"\\nğŸ“Š Expansion: {len(expanded_4)} â†’ Filtering: {len(filtered_4)}\")\n",
    "print(f\"   Before: {', '.join(expanded_4)}\")\n",
    "print(f\"   After:  {', '.join(filtered_4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdf13c7",
   "metadata": {},
   "source": [
    "### ğŸ”¹ Synset 5: \"sweep\" (verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5815b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate synset 5: sweep\n",
    "synset_5 = synsets_to_translate[3]\n",
    "\n",
    "print(f\"ğŸ”„ Translating: {', '.join(synset_5['synonyms'])}\")\n",
    "print(f\"   Definition: {synset_5['definition']}\\n\")\n",
    "\n",
    "result_5 = pipeline.translate_synset(synset_5)\n",
    "\n",
    "filtering_payload_5 = result_5[\"payload\"][\"filtering\"]\n",
    "expansion_payload_5 = result_5[\"payload\"][\"expansion\"]\n",
    "\n",
    "print(\"\\nâœ… Translation complete!\")\n",
    "print(f\"   Filtered: {', '.join(filtering_payload_5['filtered_synonyms'])}\")\n",
    "print(f\"   Confidence: {filtering_payload_5['confidence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f9223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis for synset 5\n",
    "print(\"=\" * 70)\n",
    "print(\"SYNSET 5 ANALYSIS: sweep\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "confidence_by_word_5 = filtering_payload_5.get(\"confidence_by_word\", {})\n",
    "if confidence_by_word_5:\n",
    "    print(\"\\nğŸ¯ Per-word confidence:\")\n",
    "    for word, conf in confidence_by_word_5.items():\n",
    "        emoji_map = {\"high\": \"ğŸŸ¢\", \"medium\": \"ğŸŸ¡\", \"low\": \"ğŸ”´\"}\n",
    "        print(f\"   {emoji_map.get(conf, 'âšª')} {word:20} â†’ {conf}\")\n",
    "\n",
    "removed_5 = filtering_payload_5.get(\"removed\", [])\n",
    "print(f\"\\nâŒ Removed: {len(removed_5)} candidates\")\n",
    "if removed_5:\n",
    "    for item in removed_5:\n",
    "        print(f\"   â€¢ {item.get('word', '?'):20} â†’ {item.get('reason', 'No reason')}\")\n",
    "\n",
    "expanded_5 = expansion_payload_5.get(\"expanded_synonyms\", [])\n",
    "filtered_5 = filtering_payload_5.get(\"filtered_synonyms\", [])\n",
    "print(f\"\\nğŸ“Š Expansion: {len(expanded_5)} â†’ Filtering: {len(filtered_5)}\")\n",
    "print(f\"   Before: {', '.join(expanded_5)}\")\n",
    "print(f\"   After:  {', '.join(filtered_5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5aaa6a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š Comparative Summary\n",
    "\n",
    "Compare the results across all 5 synsets to see how the pipeline handles different word types and semantic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0d6399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison of all 5 synsets\n",
    "print(\"=\" * 90)\n",
    "print(\"COMPARATIVE SUMMARY: All 5 Synsets\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "results = [\n",
    "    (\"institution\", synset_input, filtering_payload, expansion_payload),\n",
    "    (\"condiment\", synset_2, filtering_payload_2, expansion_payload_2),\n",
    "    (\"scatter/sprinkle\", synset_3, filtering_payload_3, expansion_payload_3),\n",
    "    (\"pick/pluck\", synset_4, filtering_payload_4, expansion_payload_4),\n",
    "    (\"sweep\", synset_5, filtering_payload_5, expansion_payload_5),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Synset':<18} {'POS':<5} {'Expanded':<10} {'Filtered':<10} {'Removed':<10} {'Confidence':<12}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for name, synset, filt_payload, exp_payload in results:\n",
    "    pos = synset['pos']\n",
    "    expanded_count = len(exp_payload.get('expanded_synonyms', []))\n",
    "    filtered_count = len(filt_payload.get('filtered_synonyms', []))\n",
    "    removed_count = len(filt_payload.get('removed', []))\n",
    "    confidence = filt_payload.get('confidence', 'N/A')\n",
    "    \n",
    "    print(f\"{name:<18} {pos:<5} {expanded_count:<10} {filtered_count:<10} {removed_count:<10} {confidence:<12}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Calculate statistics\n",
    "total_expanded = sum(len(r[3].get('expanded_synonyms', [])) for r in results)\n",
    "total_filtered = sum(len(r[2].get('filtered_synonyms', [])) for r in results)\n",
    "total_removed = sum(len(r[2].get('removed', [])) for r in results)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Total candidates expanded: {total_expanded}\")\n",
    "print(f\"âœ… Total candidates filtered: {total_filtered}\")\n",
    "print(f\"âŒ Total candidates removed: {total_removed}\")\n",
    "print(f\"ğŸ“‰ Average removal rate: {(total_removed/total_expanded*100):.1f}%\")\n",
    "\n",
    "# Confidence distribution\n",
    "high_conf = sum(1 for r in results if r[2].get('confidence') == 'high')\n",
    "medium_conf = sum(1 for r in results if r[2].get('confidence') == 'medium')\n",
    "low_conf = sum(1 for r in results if r[2].get('confidence') == 'low')\n",
    "\n",
    "print(f\"\\nğŸ¯ Overall confidence distribution:\")\n",
    "print(f\"   ğŸŸ¢ High: {high_conf}/5 synsets ({high_conf/5*100:.0f}%)\")\n",
    "print(f\"   ğŸŸ¡ Medium: {medium_conf}/5 synsets ({medium_conf/5*100:.0f}%)\")\n",
    "print(f\"   ğŸ”´ Low: {low_conf}/5 synsets ({low_conf/5*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ecc0fd",
   "metadata": {},
   "source": [
    "## ğŸ‰ Pass 3 Key Findings - Calibrated Prompts\n",
    "\n",
    "The calibrated filtering prompt successfully achieves our goals:\n",
    "\n",
    "1. **Balanced Approach**: The pipeline accepts both strict synonyms and culturally appropriate variants\n",
    "   - Example: **\"ustanova\" now KEPT** for \"institution\" (was removed in Pass 2)\n",
    "   - Also includes: \"sediÅ¡te\", \"kancelarija\", \"zgrada\" (culturally typical terms)\n",
    "   \n",
    "2. **Quality Filtering**: Still removes genuinely problematic translations\n",
    "   - \"centar\" (too generic)\n",
    "   - \"institucija\" (too abstract for physical building sense)\n",
    "   - \"zaÄinski dodatak\" (unnatural compound phrase)\n",
    "   - \"Äistiti\" (too general for \"sweep\")\n",
    "   \n",
    "3. **Per-Word Confidence**: Provides granular quality metrics\n",
    "   - **100% high-confidence synsets** (5/5 synsets)\n",
    "   - Individual word-level confidence for nuanced evaluation\n",
    "   \n",
    "4. **Optimal Removal Rate**: **37.7% removal rate** indicates:\n",
    "   - Not too strict (preserving valid variants like \"ustanova\")\n",
    "   - Not too lenient (filtering out poor matches and compounds)\n",
    "   - Right in the \"Goldilocks zone\" between Pass 1 (39.6%) and Pass 2 (67.9%)\n",
    "   \n",
    "5. **Good Synset Size**: **Average 6.4 synonyms per synset**\n",
    "   - Large enough to be useful for lexicographers\n",
    "   - Matches Pass 1 quality while maintaining Pass 2's filtering rigor\n",
    "   \n",
    "6. **Aspect Handling**: Successfully captures both perfective and imperfective forms\n",
    "   - \"rasprÅ¡iti/rasprÅ¡ivati\", \"posipati/posipavati\", \"metati/metnuti\", \"pometati/pometiti\"\n",
    "\n",
    "**ğŸ¯ Calibration Success**: The revised prompt achieves the balance we were seeking!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f3c6ad",
   "metadata": {},
   "source": [
    "## ğŸ“– Definition Translations\n",
    "\n",
    "Let's examine how the pipeline translated the English definitions (glosses) into Serbian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1b2936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show definition translations for all 5 synsets - comparing all three versions\n",
    "results_with_synsets = [\n",
    "    (\"institution\", synset_input, result, pairs[0]),\n",
    "    (\"condiment\", synset_2, result_2, pairs[1]),\n",
    "    (\"scatter/sprinkle\", synset_3, result_3, pairs[2]),\n",
    "    (\"pick/pluck\", synset_4, result_4, pairs[3]),\n",
    "    (\"sweep\", synset_5, result_5, pairs[4]),\n",
    "]\n",
    "\n",
    "for name, synset, res, serbian_pair in results_with_synsets:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{name.upper()}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    definition_payload = res[\"payload\"][\"definition\"]\n",
    "    \n",
    "    print(f\"\\nğŸ‡¬ğŸ‡§ English (source):\")\n",
    "    print(f\"   {synset['definition']}\")\n",
    "    \n",
    "    print(f\"\\nğŸ¤– Serbian (pipeline translation):\")\n",
    "    print(f\"   {definition_payload['definition_translation']}\")\n",
    "    \n",
    "    print(f\"\\nğŸ‘¤ Serbian (human translation from existing WordNet):\")\n",
    "    print(f\"   {serbian_pair['serbian_definition']}\")\n",
    "    \n",
    "    # Show how similar they are\n",
    "    pipeline_def = definition_payload['definition_translation'].lower()\n",
    "    human_def = serbian_pair['serbian_definition'].lower()\n",
    "    \n",
    "    if pipeline_def == human_def:\n",
    "        print(f\"\\n   âœ… Identical translations!\")\n",
    "    elif pipeline_def in human_def or human_def in pipeline_def:\n",
    "        print(f\"\\n   âš ï¸ Partially overlapping translations\")\n",
    "    else:\n",
    "        print(f\"\\n   â„¹ï¸ Different translations (both may be valid)\")\n",
    "    \n",
    "    if definition_payload.get('notes'):\n",
    "        print(f\"\\nğŸ“ Pipeline notes: {definition_payload['notes']}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3905e002",
   "metadata": {},
   "source": [
    "## ğŸ” Comparison with Existing Serbian WordNet\n",
    "\n",
    "Since our examples come from aligned Serbian-English synset pairs, we can compare our pipeline's output with the existing human-created Serbian WordNet synsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a0aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare our pipeline output with existing Serbian WordNet\n",
    "comparison_data = [\n",
    "    (\"institution\", result, filtering_payload, pairs[0]),\n",
    "    (\"condiment\", result_2, filtering_payload_2, pairs[1]),\n",
    "    (\"scatter/sprinkle\", result_3, filtering_payload_3, pairs[2]),\n",
    "    (\"pick/pluck\", result_4, filtering_payload_4, pairs[3]),\n",
    "    (\"sweep\", result_5, filtering_payload_5, pairs[4]),\n",
    "]\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"SYNSET COMPARISON: Pipeline vs Existing Serbian WordNet\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "total_overlap = 0\n",
    "total_existing = 0\n",
    "total_our = 0\n",
    "\n",
    "for name, res, filt_payload, serbian_pair in comparison_data:\n",
    "    print(f\"\\n{'=' * 90}\")\n",
    "    print(f\"{name.upper()}\")\n",
    "    print(f\"{'=' * 90}\")\n",
    "    \n",
    "    # Our output\n",
    "    our_words = set(filt_payload['filtered_synonyms'])\n",
    "    our_confidence = filt_payload['confidence']\n",
    "    \n",
    "    # Existing Serbian WordNet\n",
    "    their_words = set(serbian_pair['serbian_synonyms'])\n",
    "    their_definition = serbian_pair['serbian_definition']\n",
    "    \n",
    "    # Calculate overlap\n",
    "    overlap = our_words & their_words\n",
    "    only_ours = our_words - their_words\n",
    "    only_theirs = their_words - our_words\n",
    "    \n",
    "    print(f\"\\nğŸ†• Our pipeline ({len(our_words)} synonyms, confidence: {our_confidence}):\")\n",
    "    print(f\"   {', '.join(sorted(our_words))}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“š Existing Serbian WordNet ({len(their_words)} synonyms):\")\n",
    "    print(f\"   {', '.join(sorted(their_words))}\")\n",
    "    print(f\"   Definition: {their_definition}\")\n",
    "    \n",
    "    print(f\"\\nğŸ”„ Overlap analysis:\")\n",
    "    print(f\"   âœ… Matches ({len(overlap)}): {', '.join(sorted(overlap)) if overlap else 'None'}\")\n",
    "    print(f\"   ğŸ†• Only in pipeline ({len(only_ours)}): {', '.join(sorted(only_ours)) if only_ours else 'None'}\")\n",
    "    print(f\"   ğŸ“š Only in existing ({len(only_theirs)}): {', '.join(sorted(only_theirs)) if only_theirs else 'None'}\")\n",
    "    \n",
    "    if len(their_words) > 0:\n",
    "        match_rate = len(overlap) / len(their_words) * 100\n",
    "        print(f\"   ğŸ“Š Match rate: {len(overlap)}/{len(their_words)} ({match_rate:.1f}%)\")\n",
    "    \n",
    "    # Accumulate totals\n",
    "    total_overlap += len(overlap)\n",
    "    total_existing += len(their_words)\n",
    "    total_our += len(our_words)\n",
    "\n",
    "print(f\"\\n{'=' * 90}\")\n",
    "print(\"OVERALL STATISTICS\")\n",
    "print(f\"{'=' * 90}\")\n",
    "print(f\"\\nğŸ“Š Total synonyms:\")\n",
    "print(f\"   Our pipeline: {total_our}\")\n",
    "print(f\"   Existing WordNet: {total_existing}\")\n",
    "print(f\"   Matches: {total_overlap}\")\n",
    "print(f\"   Overall match rate: {total_overlap}/{total_existing} ({total_overlap/total_existing*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d02142",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Interpretation of Comparison Results\n",
    "\n",
    "The comparison reveals several important insights:\n",
    "\n",
    "**Expected Differences:**\n",
    "- **Different valid translations**: Both our pipeline and human translators may choose different but equally valid synonyms\n",
    "- **Aspectual variants**: Serbian verbs have perfective/imperfective forms - we may include both while existing synset has one\n",
    "- **Granularity choices**: Our pipeline may be more or less specific in synonym selection\n",
    "\n",
    "**Pipeline Advantages:**\n",
    "- **Comprehensive coverage**: May find additional valid synonyms that humans didn't include\n",
    "- **Consistency**: Follows systematic rules across all synsets\n",
    "- **Per-word confidence**: Provides quality metrics for each synonym\n",
    "\n",
    "**Human Advantages:**\n",
    "- **Cultural nuance**: Native speakers may prefer certain expressions\n",
    "- **Domain expertise**: May include domain-specific terms\n",
    "- **Established usage**: Reflects actual WordNet community decisions\n",
    "\n",
    "The goal is not 100% match rate, but rather to **complement** existing resources with high-quality, confidence-scored suggestions that lexicographers can review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549b671e",
   "metadata": {},
   "source": [
    "## ğŸ”„ Iterative Expansion Feature\n",
    "\n",
    "The pipeline now uses **iterative expansion** - running the expansion stage multiple times until no new synonyms appear (or reaching a maximum of 5 iterations). This ensures comprehensive synonym coverage since LLM outputs can vary between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cde3e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show iterative expansion details for all 5 synsets\n",
    "print(\"=\" * 90)\n",
    "print(\"ITERATIVE EXPANSION ANALYSIS\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "for name, synset, res, serbian_pair in results_with_synsets:\n",
    "    expansion_payload = res[\"payload\"][\"expansion\"]\n",
    "    \n",
    "    print(f\"\\n{name.upper()}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    iterations_run = expansion_payload.get(\"iterations_run\", 1)\n",
    "    converged = expansion_payload.get(\"converged\", False)\n",
    "    synonym_provenance = expansion_payload.get(\"synonym_provenance\", {})\n",
    "    \n",
    "    print(f\"ğŸ”„ Iterations: {iterations_run}\")\n",
    "    print(f\"âœ“ Converged: {'Yes' if converged else 'No (hit max limit)'}\")\n",
    "    print(f\"ğŸ“Š Total unique synonyms: {len(expansion_payload['expanded_synonyms'])}\")\n",
    "    \n",
    "    # Count synonyms by iteration\n",
    "    if synonym_provenance:\n",
    "        iteration_counts = {}\n",
    "        for syn, iter_num in synonym_provenance.items():\n",
    "            iteration_counts[iter_num] = iteration_counts.get(iter_num, 0) + 1\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ Synonyms found per iteration:\")\n",
    "        for iter_num in sorted(iteration_counts.keys()):\n",
    "            count = iteration_counts[iter_num]\n",
    "            if iter_num == 0:\n",
    "                print(f\"   Initial: {count} synonyms\")\n",
    "            else:\n",
    "                print(f\"   Iteration {iter_num}: {count} new synonyms\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"This iterative approach ensures comprehensive coverage while\")\n",
    "print(\"stopping early when the LLM has exhausted its synonym knowledge.\")\n",
    "print(\"=\" * 90)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
